import numpy as np
import matplotlib.pyplot as plt


# y = mx + c
# Function for Calculating the Coefficients Values
def estimate_coeff(x, y):
    # number of observations / points
    n = np.size(x)

    # mean of x and y vector
    m_x, m_y = np.mean(x), np.mean(y)

    # Calculate the Cross Deviation and Deviation about x
    SS_xy = np.sum(y * x) - n * m_y * m_x
    SS_xx = np.sum(x * x) - n * m_x * m_x

    # Calculating the regression Coefficients
    b_1 = SS_xy / SS_xx  # y = mx + c i.e this b_1 is representing x here in the statement
    b_0 = m_y - b_1 * m_x  # y = mx + c i.e this b_0 is representing c here in the statement

    return (b_0, b_1)


# Function for Plotting the Regression Line
def plot_regression_line(x, y, b):
    # Plotting the actual points as scatter plot
    plt.scatter(x, y, color="m", marker="o", s=30)

    # predicted response vector
    y_pred = b[0] + b[1] * x  # y = mx + c

    # plotting the regression line
    plt.plot(x, y_pred, color="g")

    # putting the labels in the Axis
    plt.xlabel("X")
    plt.ylabel("Y")

    # function to show the Scatter plot
    plt.show()


# Main Function for Executing the Script where all other Functions will be Called in Main Function:
def main():
    # Observations / Data
    x = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    y = np.array([1, 3, 2, 5, 7, 8, 8, 9, 10, 12])

    # Call estimate_coeff Function
    b = estimate_coeff(x, y)
    print("Estimated Coefficients Values:\nb_0 = {}\nb_1 = {}".format(b[0], b[1]))

    # Calling the Plot Regression Line Function
    plot_regression_line(x, y, b)


## Calling the Main Function to start the Execution of Algorithm

if __name__ == "__main__":
    main()

# In[7]:


# Multiple Linear Regression: Its a Regresison where we will have more than one independent variable on which the algorithm
# creates its plotting area and try to draw a Regression line with covering max no of points

import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model, metrics

# Loading of the Data Set for the Use and the inbuilt DataSet is BOSTON DataSet which will be used
boston = datasets.load_boston(return_X_y=False)

# Defining the Feature matrix(X) and response vector (Y)
X = boston.data
y = boston.target

# Split the Data into Training and Testing Sets
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)

# Create the Linear Regression Object for Calculation and execution of the Data
reg = linear_model.LinearRegression()

# Train the model using the training sets
reg.fit(X_train, y_train)

# Regression Coefficients calling
print("Coefficients from Linear Regression for Training Data are:\n", reg.coef_)

# Creating a Variance Score: 1 means the perfect prediction
print("Variance Score is {}: ", format(reg.score(X_test, y_test)))

# Do the Plotting for Residual Error
## Set the Plotting Style for Graphs and Figures
plt.style.use("fivethirtyeight")

# Plot the Residual Errors in the Training Data Set created in X_train
plt.scatter(reg.predict(X_train), reg.predict(X_train) - y_train, color="green", s=10, label="Training Data")

# Plot the Residual Errors in the Testing Data Set created in X_test
plt.scatter(reg.predict(X_test), reg.predict(X_test) - y_test, color="blue", s=10, label="Testing Data")

# Plot the Line for Zero '0' Residual Error
plt.hlines(y=0, xmin=0, xmax=50, linewidth=2)

# Plot the Legend on Graph
plt.legend(loc="upper right")

# plotting the title
plt.title("Residual Errors")

# Function to show plot on plane
plt.show()

# In[11]:


# Polynomial Regression: It is used where the Linear Regression Line is not able to cover up all the data points in the graph
# using Data, so in that cases we will plot polynomial linear Regression to plot the Data.

import operator

import numpy as np
import matplotlib.pyplot as plt

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import PolynomialFeatures

np.random.seed(0)
x = 2 - 3 * np.random.normal(0, 1, 20)
y = x - 2 * (x ** 2) + 0.5 * (x ** 3) + np.random.normal(-3, 3, 20)

# Transforming the Data to include axis
x = x[:, np.newaxis]
y = y[:, np.newaxis]

polynomial_features = PolynomialFeatures(degree=20)
x_poly = polynomial_features.fit_transform(x)

model = LinearRegression()
model.fit(x_poly, y)
y_poly_pred = model.predict(x_poly)

# RMSE
rmse = np.sqrt(mean_squared_error(y, y_poly_pred))
r2 = r2_score(y, y_poly_pred)
print(rmse)
print(r2)

plt.scatter(x, y, s=10)

# Sort the values of x before we plot the line plot for X axis
sort_axis = operator.itemgetter(0)
sorted_zip = sorted(zip(x, y_poly_pred), key=sort_axis)
x, y_poly_pred = zip(*sorted_zip)
plt.plot(x, y_poly_pred, color="m")
plt.show()
